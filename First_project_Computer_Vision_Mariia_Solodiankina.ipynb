{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "First project Computer Vision - Mariia Solodiankina.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MariiaS/computer_vision/blob/main/First_project_Computer_Vision_Mariia_Solodiankina.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jess476v1hVF"
      },
      "source": [
        "Your first project submission must contain:\n",
        "\n",
        "All the necessary instructions to execute your project.\n",
        "The modifications of facedetect to detect your face from the camera on google Colab\n",
        "The modifications to make faster the prediction (defining the new rectangle around the first detected face)\n",
        "Your code must be:\n",
        "\n",
        "cleaned-up of useless portions\n",
        "commented (both for the existing code and the code you have added)\n",
        "improved: the face detection must be done in the neighborhood of the face detect location in the previous frame and not in the full frame\n",
        "Well written, modular, robust, ...."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBGf6xh4Hg2K"
      },
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import io\n",
        "import cv2"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohdgQWXLjZVR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6162a7b4-5ccd-4a36-ffc2-35a58796fa50"
      },
      "source": [
        "cv2.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'4.1.2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XS0UfPTMIAjH"
      },
      "source": [
        "from google.colab.output import eval_js\n",
        "\n",
        "def VideoCapture():\n",
        "  js = Javascript('''\n",
        "    async function create(){\n",
        "      div = document.createElement('div');\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      video = document.createElement('video');\n",
        "      video.setAttribute('playsinline', '');\n",
        "\n",
        "      div.appendChild(video);\n",
        "      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n",
        "      video.srcObject = stream;\n",
        "\n",
        "      await video.play();\n",
        "\n",
        "      canvas =  document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "\n",
        "      div_out = document.createElement('div');\n",
        "      document.body.appendChild(div_out);\n",
        "      img = document.createElement('img');\n",
        "      div_out.appendChild(img);\n",
        "    }\n",
        "\n",
        "    async function capture(){\n",
        "        return await new Promise(function(resolve, reject){\n",
        "            pendingResolve = resolve;\n",
        "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "            result = canvas.toDataURL('image/jpeg', 0.20);\n",
        "\n",
        "            pendingResolve(result);\n",
        "        })\n",
        "    }\n",
        "\n",
        "    function showimg(imgb64){\n",
        "        img.src = \"data:image/jpg;base64,\" + imgb64;\n",
        "    }\n",
        "\n",
        "  ''')\n",
        "  display(js)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRSMs81vI1kW"
      },
      "source": [
        "def b64_to_bytes(byte):\n",
        "  jpeg = b64decode(byte.split(',')[1])\n",
        "  im = Image.open(io.BytesIO(jpeg))\n",
        "  return np.array(im)\n",
        "\n",
        "def bytes_to_b64(image):\n",
        "  image = Image.fromarray(image)\n",
        "  buffer = io.BytesIO()\n",
        "  image.save(buffer, 'jpeg')\n",
        "  buffer.seek(0)\n",
        "  x = b64encode(buffer.read()).decode('utf-8')\n",
        "  return x"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvm1YoLZKNow"
      },
      "source": [
        "# Load cascades using v2.CascadeClassifier("
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCmab-skYzUf"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNPTfhWrY5R8"
      },
      "source": [
        "face_cascades = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7ayo005z4ij"
      },
      "source": [
        "Reduce the computation time of Facedetect"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbb0ctnDfJbx"
      },
      "source": [
        "def detect_faces(img, cascades):\n",
        "  # trnasform to gray\n",
        "  # use cascades to detect faces\n",
        "  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  faces = cascades.detectMultiScale(gray, 1.3, 4)\n",
        "  return faces"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yVIWlHefU-m"
      },
      "source": [
        "# This function computes the sub region in which we would want to detect faces\n",
        "# Params:\n",
        "# previous: The previous face bounding box (x, y, w, h)\n",
        "# image heigh and img_width: Self explaining and used to insure boundary constraints\n",
        "# margin: How mach farther from the previous bounding box you would like to search for faces in the current frame \n",
        "def compute_optimized_search_region(previous, img_height, img_width, margin):\n",
        "  x_new = previous[0] - margin\n",
        "  y_new = previous[1] - margin\n",
        "  w_new = (previous[0]+previous[2]+margin) - x_new\n",
        "  h_new = (previous[1]+previous[3]+margin) - y_new\n",
        "  # Don't forget to insure boundary constraints:\n",
        "  x_new, y_new = max(x_new, 0), max(y_new, 0)\n",
        "  w_new, h_new = min(w_new, img_width), min(h_new, img_height)\n",
        "  \n",
        "  return (x_new, y_new, w_new, h_new)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZHxqyBNNdKD"
      },
      "source": [
        "VideoCapture()\n",
        "eval_js('create()')\n",
        "\n",
        "\n",
        "byte = eval_js('capture()')\n",
        "im = b64_to_bytes(byte)\n",
        "(img_height, img_width) = im.shape[0], im.shape[1]\n",
        "\n",
        "previous_bbox = None\n",
        "margin = 50\n",
        "\n",
        "\n",
        "while True:\n",
        "  byte = eval_js('capture()')\n",
        "  im = b64_to_bytes(byte)\n",
        "  region_to_use = im\n",
        "\n",
        "  if previous_bbox is not None: # If we have detected a face in the previous frame\n",
        "    # Use the previous bounding box to compute the sub region in which you would like to seach for faces\n",
        "    x_new, y_new, w_new, h_new = compute_optimized_search_region(previous_bbox, img_height, img_width, margin)\n",
        "    # The new sub region\n",
        "    # Draw a red rectangle around the sub region in the 'im' image (because it's the one we will plot finally)\n",
        "    region_to_use = im[x_new: x_new+w_new, y_new:y_new+h_new]\n",
        "    cv2.rectangle(im, (x_new, y_new), (x_new+w_new, y_new+h_new), (255,0),5)\n",
        "\n",
        "  # detect faces in the subr egion to use\n",
        "  faces = detect_faces(region_to_use, face_cascades)\n",
        "  \n",
        "  if len(faces) == 1:\n",
        "    face = faces[0]\n",
        "    # Update the face bounding box. Be careful: The coordinates should be relative the full image 'im' and not relative to the 'region_to_use'\n",
        "    x,y,w,h = face if previous_bbox is None else (x_new+face[0], y_new+face[1], face[2], face[3])\n",
        "    previous_bbox = (x,y,w,h)\n",
        "    cv2.rectangle(im, (x,y), (x+w, y+h), (0, 255), 2)\n",
        "    # Draw a green rectangle around the detected face in the original image 'im'\n",
        "  else: \n",
        "    previous_bbox = None # If we did not detect any face or mor ethan one face we just repeat the search using the entire image in the next frame\n",
        "\n",
        "  eval_js('showimg(\"{}\")'.format(bytes_to_b64(im))) # We convert our image with bounding boxes to base64 and plot it using JS "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}