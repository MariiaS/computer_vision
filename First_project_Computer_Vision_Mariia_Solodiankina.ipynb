{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "First project Computer Vision - Mariia Solodiankina.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MariiaS/computer_vision/blob/main/First_project_Computer_Vision_Mariia_Solodiankina.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jess476v1hVF"
      },
      "source": [
        "Your first project submission must contain:\n",
        "\n",
        "All the necessary instructions to execute your project.\n",
        "The modifications of facedetect to detect your face from the camera on google Colab\n",
        "The modifications to make faster the prediction (defining the new rectangle around the first detected face)\n",
        "Your code must be:\n",
        "\n",
        "cleaned-up of useless portions\n",
        "commented (both for the existing code and the code you have added)\n",
        "improved: the face detection must be done in the neighborhood of the face detect location in the previous frame and not in the full frame\n",
        "Well written, modular, robust, ...."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBGf6xh4Hg2K"
      },
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import io\n",
        "import cv2"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohdgQWXLjZVR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4da95179-f3e4-4a70-823c-95f6b469eea5"
      },
      "source": [
        "cv2.__version__ #checking the version of cv2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'4.1.2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XS0UfPTMIAjH"
      },
      "source": [
        "# function to stream video from the camera in the real-time\n",
        "\n",
        "def VideoCapture():\n",
        "  js = Javascript('''\n",
        "    async function create(){\n",
        "      div = document.createElement('div');\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      video = document.createElement('video');\n",
        "      video.setAttribute('playsinline', '');\n",
        "\n",
        "      div.appendChild(video);\n",
        "      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n",
        "      video.srcObject = stream;\n",
        "\n",
        "      await video.play();\n",
        "\n",
        "      canvas =  document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "\n",
        "      div_out = document.createElement('div');\n",
        "      document.body.appendChild(div_out);\n",
        "      img = document.createElement('img');\n",
        "      div_out.appendChild(img);\n",
        "    }\n",
        "\n",
        "    async function capture(){\n",
        "        return await new Promise(function(resolve, reject){\n",
        "            pendingResolve = resolve;\n",
        "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "            result = canvas.toDataURL('image/jpeg', 0.20);\n",
        "\n",
        "            pendingResolve(result);\n",
        "        })\n",
        "    }\n",
        "\n",
        "    function showimg(imgb64){\n",
        "        img.src = \"data:image/jpg;base64,\" + imgb64;\n",
        "    }\n",
        "\n",
        "  ''')\n",
        "  display(js)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRSMs81vI1kW"
      },
      "source": [
        "def b64_to_bytes(byte):\n",
        "  jpeg = b64decode(byte.split(',')[1])\n",
        "  im = Image.open(io.BytesIO(jpeg))\n",
        "  return np.array(im)\n",
        "\n",
        "def bytes_to_b64(image):\n",
        "  image = Image.fromarray(image)\n",
        "  buffer = io.BytesIO()\n",
        "  image.save(buffer, 'jpeg')\n",
        "  buffer.seek(0)\n",
        "  x = b64encode(buffer.read()).decode('utf-8')\n",
        "  return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvm1YoLZKNow"
      },
      "source": [
        "# Load face cascades using v2.CascadeClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCmab-skYzUf",
        "outputId": "9a3a420e-fb3d-4cd1-b447-bc9934e74449"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml\n",
        "!ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-14 21:27:29--  https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 930127 (908K) [text/plain]\n",
            "Saving to: ‘haarcascade_frontalface_default.xml’\n",
            "\n",
            "haarcascade_frontal 100%[===================>] 908.33K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2021-02-14 21:27:30 (13.8 MB/s) - ‘haarcascade_frontalface_default.xml’ saved [930127/930127]\n",
            "\n",
            "haarcascade_frontalface_default.xml  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNPTfhWrY5R8"
      },
      "source": [
        "face_cascades = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7ayo005z4ij"
      },
      "source": [
        "Reduce the computation time of Facedetect"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbb0ctnDfJbx"
      },
      "source": [
        "def detect_faces(img, cascades):\n",
        "  # trnasform to gray\n",
        "  # use cascades to detect faces\n",
        "  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  faces = cascades.detectMultiScale(gray, 1.3, 4)\n",
        "  return faces"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yVIWlHefU-m"
      },
      "source": [
        "# This function computes the sub region in which we would want to detect faces\n",
        "# Params:\n",
        "# previous: The previous face bounding box (x, y, w, h)\n",
        "# image heigh and img_width: Self explaining and used to insure boundary constraints\n",
        "# margin: How mach farther from the previous bounding box you would like to search for faces in the current frame \n",
        "def compute_optimized_search_region(previous, img_height, img_width, margin):\n",
        "  x_new = previous[0] - margin\n",
        "  y_new = previous[1] - margin\n",
        "  w_new = (previous[0]+previous[2]+margin) - x_new\n",
        "  h_new = (previous[1]+previous[3]+margin) - y_new\n",
        "  # Don't forget to insure boundary constraints:\n",
        "  x_new, y_new = max(x_new, 0), max(y_new, 0)\n",
        "  w_new, h_new = min(w_new, img_width), min(h_new, img_height)\n",
        "  \n",
        "  return (x_new, y_new, w_new, h_new)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZHxqyBNNdKD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ee76b561-5108-49cf-fb8b-630a13d2a541"
      },
      "source": [
        "VideoCapture()\n",
        "eval_js('create()')\n",
        "\n",
        "\n",
        "byte = eval_js('capture()')\n",
        "im = b64_to_bytes(byte)\n",
        "(img_height, img_width) = im.shape[0], im.shape[1]\n",
        "\n",
        "previous_bbox = None\n",
        "margin = 50\n",
        "\n",
        "\n",
        "while True:\n",
        "  byte = eval_js('capture()')\n",
        "  im = b64_to_bytes(byte)\n",
        "  region_to_use = im\n",
        "\n",
        "  if previous_bbox is not None: # If we have detected a face in the previous frame\n",
        "    # Use the previous bounding box to compute the sub region in which you would like to seach for faces\n",
        "    x_new, y_new, w_new, h_new = compute_optimized_search_region(previous_bbox, img_height, img_width, margin)\n",
        "    # The new sub region\n",
        "    # Draw a red rectangle around the sub region in the 'im' image (because it's the one we will plot finally)\n",
        "    region_to_use = im[x_new: x_new+w_new, y_new:y_new+h_new]\n",
        "    # (255,0) - red color\n",
        "    cv2.rectangle(im, (x_new, y_new), (x_new+w_new, y_new+h_new), (255,0), 5)\n",
        "\n",
        "  # detect faces in the subr egion to use\n",
        "  faces = detect_faces(region_to_use, face_cascades)\n",
        "  \n",
        "  if len(faces) == 1:\n",
        "    face = faces[0]\n",
        "    # Update the face bounding box. Be careful: The coordinates should be relative the full image 'im' and not relative to the 'region_to_use'\n",
        "    x,y,w,h = face if previous_bbox is None else (x_new+face[0], y_new+face[1], face[2], face[3])\n",
        "    previous_bbox = (x,y,w,h)\n",
        "    # (0, 255) - green\n",
        "    cv2.rectangle(im, (x,y), (x+w, y+h), (0, 255), 2)\n",
        "    # Draw a green rectangle around the detected face in the original image 'im'\n",
        "  else: \n",
        "    previous_bbox = None # If we did not detect any face or mor ethan one face we just repeat the search using the entire image in the next frame\n",
        "\n",
        "  eval_js('showimg(\"{}\")'.format(bytes_to_b64(im))) # We convert our image with bounding boxes to base64 and plot it using JS "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function create(){\n",
              "      div = document.createElement('div');\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      video = document.createElement('video');\n",
              "      video.setAttribute('playsinline', '');\n",
              "\n",
              "      div.appendChild(video);\n",
              "      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n",
              "      video.srcObject = stream;\n",
              "\n",
              "      await video.play();\n",
              "\n",
              "      canvas =  document.createElement('canvas');\n",
              "      canvas.width = video.videoWidth;\n",
              "      canvas.height = video.videoHeight;\n",
              "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "\n",
              "      div_out = document.createElement('div');\n",
              "      document.body.appendChild(div_out);\n",
              "      img = document.createElement('img');\n",
              "      div_out.appendChild(img);\n",
              "    }\n",
              "\n",
              "    async function capture(){\n",
              "        return await new Promise(function(resolve, reject){\n",
              "            pendingResolve = resolve;\n",
              "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "            result = canvas.toDataURL('image/jpeg', 0.20);\n",
              "\n",
              "            pendingResolve(result);\n",
              "        })\n",
              "    }\n",
              "\n",
              "    function showimg(imgb64){\n",
              "        img.src = \"data:image/jpg;base64,\" + imgb64;\n",
              "    }\n",
              "\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-c3d96ca003d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mbyte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'capture()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb64_to_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mregion_to_use\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}